operatorNamespace: rook-ceph
clusterName: rook-ceph

pspEnable: false

monitoring:
  enabled: true
  createPrometheusRules: false

toolbox:
  enabled: true
  resources:
    limits:
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 256Mi
  priorityClassName: system-cluster-critical

cephClusterSpec:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.3
    allowUnsupported: false

  cephConfig:
    global:
      # This cluster currently runs OSDs on 2 hosts, so the default replicated pool size must be 2.
      # Pools without an explicit size (e.g. `.mgr`) inherit these defaults.
      osd_pool_default_size: "2"
      osd_pool_default_min_size: "1"

  dataDirHostPath: /var/lib/rook

  mon:
    count: 3
    allowMultiplePerNode: false

  # Keep talos-192-168-1-194 as control-plane only (mon node, no OSDs).
  placement:
    mon:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - talos-192-168-1-85
                    - talos-192-168-1-194
                    - talos-192-168-1-203

  mgr:
    count: 2
    allowMultiplePerNode: false

  dashboard:
    enabled: true
    ssl: false

  network:
    connections:
      encryption:
        enabled: false
      compression:
        enabled: false
      requireMsgr2: true

  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical

  storage:
    # Scope OSDs to dedicated storage nodes
    useAllNodes: false
    useAllDevices: false
    deviceFilter: ""
    nodes:
      - name: talos-192-168-1-85
        devices:
          - name: /dev/disk/by-id/ata-ST24000NM000C-3WD103_ZXA12R7C
          - name: /dev/disk/by-id/ata-ST24000NM000C-3WD103_ZXA0LKW9
          - name: /dev/disk/by-id/ata-ST24000NM000C-3WD103_ZXA0HS7E
      - name: talos-192-168-1-203
        devices:
          - name: /dev/disk/by-id/ata-ST24000NM000C-3WD103_ZXA0NL5D
          - name: /dev/disk/by-id/ata-ST24000NM000C-3WD103_ZXA0MZ1M
          - name: /dev/disk/by-id/ata-ST24000NM000C-3WD103_ZXA0LVM9
    config:
      databaseSizeMB: "2048"

  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30

  healthCheck:
    daemonHealth:
      mon:
        interval: 45s
      osd:
        interval: 60s
      status:
        interval: 60s

# Disable chart-managed default block/filesystem classes to avoid duplicates.
# Canonical classes are managed in storageclasses.yaml as:
# - rook-ceph-block (RWO)
# - rook-cephfs (RWX)
cephBlockPools: []
cephFileSystems: []

cephObjectStores:
  - name: objectstore
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          size: 2
      dataPool:
        failureDomain: host
        replicated:
          size: 2
        parameters:
          bulk: "true"
      preservePoolsOnDelete: true
      gateway:
        port: 80
        instances: 2
        priorityClassName: system-cluster-critical
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: "2"
            memory: 3Gi
    storageClass:
      enabled: true
      name: rook-ceph-bucket
      reclaimPolicy: Retain
      volumeBindingMode: Immediate
      parameters:
        region: us-east-1
    ingress:
      enabled: false
    route:
      enabled: false
