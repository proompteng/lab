apiVersion: agents.proompteng.ai/v1alpha1
kind: ImplementationSpec
metadata:
  name: torghut-health-report-v1
  namespace: agents
spec:
  contract:
    requiredKeys:
      - torghutNamespace
      - torghutService
      - wsService
      - flinkRestService
      - guardrailsService
  text: |
    Objective: Produce a single Torghut health report (read-only) using in-cluster HTTP endpoints only.

    Constraints:
    - Do not mutate cluster state.
    - Do not open PRs.
    - Assume kubectl is unavailable or unreliable.

    Inputs (from AgentRun parameters):
    - torghutNamespace
    - torghutService
    - wsService
    - flinkRestService
    - guardrailsService

    Steps:
    1) Build base URLs using service DNS names with `.svc.cluster.local`.
       - TORGHUT_BASE = http://<torghutService>.<torghutNamespace>.svc.cluster.local
       - WS_BASE = http://<wsService>.<torghutNamespace>.svc.cluster.local
       - FLINK_BASE = http://<flinkRestService>.<torghutNamespace>.svc.cluster.local:8081
       - GUARDRAILS_BASE = http://<guardrailsService>.<torghutNamespace>.svc.cluster.local:9108
    2) Query each endpoint and capture status/body snippets.
       - GET ${WS_BASE}/readyz and ${WS_BASE}/healthz
       - GET ${FLINK_BASE}/overview and ${FLINK_BASE}/jobs/overview
       - GET ${TORGHUT_BASE}/trading/status and ${TORGHUT_BASE}/trading/health
       - GET ${GUARDRAILS_BASE}/metrics
    3) From metrics, extract:
       - torghut_clickhouse_guardrails_clickhouse_up
       - torghut_clickhouse_guardrails_disk_free_ratio
       - torghut_clickhouse_guardrails_any_replica_readonly
       - torghut_clickhouse_guardrails_ta_signals_max_event_ts_seconds
       - torghut_clickhouse_guardrails_ta_microbars_max_window_end_seconds
       - torghut_clickhouse_guardrails_last_scrape_success
    4) Compute ages for max TA timestamps and classify freshness.
       - If market is open (US session, Mon-Fri 09:30-16:00 America/New_York) and age > 300s => degraded.
       - During closed hours report stale signal age without triggering failure by staleness alone.

    Output:
    - Produce a concise Markdown report with sections: Summary, WS, Flink, Trading, Guardrails, Freshness.
    - Conclude with a single health verdict: healthy | degraded | down.
---
apiVersion: agents.proompteng.ai/v1alpha1
kind: ImplementationSpec
metadata:
  name: torghut-v3-promotion-policy-check-v1
  namespace: agents
spec:
  contract:
    requiredKeys:
      - candidateId
      - runId
      - promotionTarget
      - policyPath
      - candidateStatePath
      - gateReportPath
      - artifactRoot
      - artifactPath
  text: |
    Objective: Enforce promotion prerequisite and rollback-readiness checks before stage promotion.

    Guardrails:
    - Read-only policy evaluation; no cluster mutation.
    - Promotion progression is denied when either prerequisite or rollback checks fail.

    Inputs:
    - candidateId, runId, promotionTarget
    - policyPath, candidateStatePath, gateReportPath
    - artifactRoot, artifactPath

    Steps:
    1) Run:
       - python services/torghut/scripts/validate_promotion_policy.py \
         --policy ${policyPath} \
         --state ${candidateStatePath} \
         --gate-report ${gateReportPath} \
         --artifact-root ${artifactRoot} \
         --promotion-target ${promotionTarget} \
         --output ${artifactPath}/promotion-policy-check.json
    2) Fail fast if `promotion_progression_allowed` is false.
    3) Publish the decision artifact path and reasons.
---
apiVersion: agents.proompteng.ai/v1alpha1
kind: ImplementationSpec
metadata:
  name: torghut-v3-promotion-policy-dry-run-v1
  namespace: agents
spec:
  contract:
    requiredKeys:
      - promotionTarget
      - policyPath
      - gateReportPath
      - artifactPath
      - simulateMissingArtifact
      - simulateStaleRollback
  text: |
    Objective: Execute dry-run harness proving policy enforcement behavior for promotion and rollback workflows.

    Constraints:
    - Must remain dry-run only.
    - Must emit structured JSON artifact to `${artifactPath}`.

    Inputs:
    - promotionTarget, policyPath, gateReportPath, artifactPath
    - simulateMissingArtifact, simulateStaleRollback

    Steps:
    1) Build optional simulation flags:
       - SIM_MISSING = `--simulate-missing-artifact` when simulateMissingArtifact is true.
       - SIM_STALE = `--simulate-stale-rollback` when simulateStaleRollback is true.
    2) Run:
       - python services/torghut/scripts/run_governance_policy_dry_run.py \
         --policy ${policyPath} \
         --gate-report ${gateReportPath} \
         --promotion-target ${promotionTarget} \
         ${SIM_MISSING} \
         ${SIM_STALE} \
         --output ${artifactPath}/governance-policy-dry-run.json
    3) Include `promotion_progression_allowed` and failure reasons in output summary.
---
apiVersion: agents.proompteng.ai/v1alpha1
kind: ImplementationSpec
metadata:
  name: torghut-gated-actuation-runner-v1
  namespace: agents
spec:
  contract:
    requiredKeys:
      - repository
      - base
      - head
      - torghutNamespace
      - gitopsPath
      - change
      - reason
      - confirm
  text: |
    Objective: Perform a single Torghut recovery actuation step via GitOps PR.

    Guardrails:
    - Do not perform any direct kubectl mutation unless explicitly requested as an emergency.
    - Only edit files under ${gitopsPath}.
    - Require confirm == ACTUATE_TORGHUT.
    - Supported changes only: pause-trading, restart-ws, suspend-ta, resume-ta.
    - PRs are mandatory for all requested changes.

    Inputs (from AgentRun parameters):
    - repository, base, head
    - torghutNamespace, gitopsPath
    - change, reason, confirm

    Steps:
    1) Validate `change` is one of: pause-trading, restart-ws, suspend-ta, resume-ta.
    2) Validate `confirm` is exactly `ACTUATE_TORGHUT`.
    3) Apply the corresponding patch under `argocd/applications/torghut`:
       - pause-trading: set TRADING_ENABLED="false" in knative-service.yaml.
       - restart-ws: bump kubectl.kubernetes.io/restartedAt in ws/deployment.yaml.
       - suspend-ta: set spec.job.state: suspended in ta/flinkdeployment.yaml.
       - resume-ta: set spec.job.state: running and bump spec.restartNonce if needed.
    4) Open a PR against ${repository} with base ${base} and head ${head}.
    5) Output PR URL and clear rollback instructions.
---
apiVersion: agents.proompteng.ai/v1alpha1
kind: ImplementationSpec
metadata:
  name: torghut-market-context-fundamentals-v1
  namespace: agents
spec:
  contract:
    requiredKeys:
      - symbol
      - domain
      - asOfUtc
      - reason
      - callbackUrl
      - requestId
  text: |
    Objective: Build a fundamentals market-context snapshot for `${symbol}` and publish it to `${callbackUrl}`.

    Guardrails:
    - Read-only research task; do not mutate cluster state.
    - Use at least 2 independent citations.
    - All timestamps must be UTC ISO-8601 strings.
    - Keep qualityScore in [0, 1].

    Inputs:
    - symbol, domain, asOfUtc, reason
    - callbackUrl, requestId

    Required output payload shape:
    {
      "symbol": "<symbol>",
      "domain": "fundamentals",
      "asOfUtc": "<ISO timestamp>",
      "sourceCount": <integer>,
      "qualityScore": <number 0..1>,
      "payload": {
        "thesis": "<short summary>",
        "valuation": { ... },
        "growth": { ... },
        "profitability": { ... },
        "balanceSheet": { ... },
        "upcomingCatalysts": [ ... ]
      },
      "citations": [
        { "source": "<publisher>", "publishedAt": "<ISO timestamp>", "url": "<https url or null>" }
      ],
      "riskFlags": [ ... ],
      "provider": "codex-spark",
      "runStatus": "succeeded",
      "requestId": "<requestId>"
    }

    Steps:
    1) Start lifecycle tracking (required before research) using repo scripts:
       - `python3 /workspace/lab/skills/market-context/scripts/market_context_run_api.py start --callback-url "${callbackUrl}" --request-id "${requestId}" --symbol "${symbol}" --domain fundamentals --reason "${reason}" --provider codex-spark --expect-status 200`
       - `python3 /workspace/lab/skills/market-context/scripts/market_context_run_api.py progress --callback-url "${callbackUrl}" --request-id "${requestId}" --seq 1 --status running --message fundamentals_collection_started --expect-status 200`
    2) Research current fundamentals for `${symbol}` (valuation, growth, profitability, balance-sheet/capital structure, near-term catalysts).
    3) Write the payload to `/tmp/market-context-fundamentals.json` using Python `json` serialization (do not handcraft shell-escaped JSON).
    4) Validate payload before submit:
       - `python3 -m json.tool /tmp/market-context-fundamentals.json >/dev/null`
       - `python3 /workspace/lab/skills/market-context/scripts/validate_market_context_payload.py --domain fundamentals --file /tmp/market-context-fundamentals.json`
    5) Build citation evidence array in `/tmp/market-context-fundamentals-evidence.json` from the same sources you used for the payload.
       - Evidence items must include at least `source`; include `publishedAt`, `url`, `headline`, and `summary` when available.
       - Submit evidence:
         `python3 /workspace/lab/skills/market-context/scripts/market_context_run_api.py evidence --callback-url "${callbackUrl}" --request-id "${requestId}" --symbol "${symbol}" --domain fundamentals --seq 2 --evidence-file /tmp/market-context-fundamentals-evidence.json --expect-status 200`
    6) Finalize and persist via lifecycle finalize endpoint:
       - `python3 /workspace/lab/skills/market-context/scripts/market_context_run_api.py finalize --callback-url "${callbackUrl}" --request-id "${requestId}" --payload-file /tmp/market-context-fundamentals.json --expect-status 200 --retries 1`
       - Require response JSON to include `"ok": true`.
---
apiVersion: agents.proompteng.ai/v1alpha1
kind: ImplementationSpec
metadata:
  name: torghut-market-context-news-v1
  namespace: agents
spec:
  contract:
    requiredKeys:
      - symbol
      - domain
      - asOfUtc
      - reason
      - callbackUrl
      - requestId
  text: |
    Objective: Build a news market-context snapshot for `${symbol}` and publish it to `${callbackUrl}`.

    Guardrails:
    - Read-only research task; do not mutate cluster state.
    - Prioritize the most recent high-signal news (last 24h, then 7d if sparse).
    - Use at least 3 citations.
    - All timestamps must be UTC ISO-8601 strings.
    - Keep qualityScore in [0, 1].

    Inputs:
    - symbol, domain, asOfUtc, reason
    - callbackUrl, requestId

    Required output payload shape:
    {
      "symbol": "<symbol>",
      "domain": "news",
      "asOfUtc": "<ISO timestamp>",
      "sourceCount": <integer>,
      "qualityScore": <number 0..1>,
      "payload": {
        "summary": "<short summary>",
        "headlines": [
          {
            "headline": "<title>",
            "publisher": "<publisher>",
            "publishedAt": "<ISO timestamp>",
            "url": "<https url or null>",
            "marketImpact": "positive|negative|mixed|neutral"
          }
        ],
        "eventRisk": "low|medium|high"
      },
      "citations": [
        { "source": "<publisher>", "publishedAt": "<ISO timestamp>", "url": "<https url or null>" }
      ],
      "riskFlags": [ ... ],
      "provider": "codex-spark",
      "runStatus": "succeeded",
      "requestId": "<requestId>"
    }

    Steps:
    1) Start lifecycle tracking (required before research) using repo scripts:
       - `python3 /workspace/lab/skills/market-context/scripts/market_context_run_api.py start --callback-url "${callbackUrl}" --request-id "${requestId}" --symbol "${symbol}" --domain news --reason "${reason}" --provider codex-spark --expect-status 200`
       - `python3 /workspace/lab/skills/market-context/scripts/market_context_run_api.py progress --callback-url "${callbackUrl}" --request-id "${requestId}" --seq 1 --status running --message news_collection_started --expect-status 200`
    2) Gather recent high-signal company/sector/macro news relevant to `${symbol}`.
    3) Write the payload to `/tmp/market-context-news.json` using Python `json` serialization (do not handcraft shell-escaped JSON).
    4) Validate payload before submit:
       - `python3 -m json.tool /tmp/market-context-news.json >/dev/null`
       - `python3 /workspace/lab/skills/market-context/scripts/validate_market_context_payload.py --domain news --file /tmp/market-context-news.json`
    5) Build citation evidence array in `/tmp/market-context-news-evidence.json` from the same sources you used for the payload.
       - Evidence items must include at least `source`; include `publishedAt`, `url`, `headline`, and `summary` when available.
       - Submit evidence:
         `python3 /workspace/lab/skills/market-context/scripts/market_context_run_api.py evidence --callback-url "${callbackUrl}" --request-id "${requestId}" --symbol "${symbol}" --domain news --seq 2 --evidence-file /tmp/market-context-news-evidence.json --expect-status 200`
    6) Finalize and persist via lifecycle finalize endpoint:
       - `python3 /workspace/lab/skills/market-context/scripts/market_context_run_api.py finalize --callback-url "${callbackUrl}" --request-id "${requestId}" --payload-file /tmp/market-context-news.json --expect-status 200 --retries 1`
       - Require response JSON to include `"ok": true`.
---
apiVersion: agents.proompteng.ai/v1alpha1
kind: ImplementationSpec
metadata:
  name: torghut-dspy-dataset-build-v1
  namespace: agents
spec:
  contract:
    requiredKeys:
      - repository
      - base
      - head
      - artifactPath
      - datasetWindow
      - universeRef
  text: |
    Objective: Build the DSPy training/evaluation dataset used by Torghut compile/eval lanes.

    Guardrails:
    - Read-only data extraction and deterministic transforms only.
    - Emit a canonical JSON dataset at `${artifactPath}/dspy-dataset.json`.
    - Include reproducibility metadata (window, filters, sampling seed, source refs).

    Inputs:
    - repository, base, head, artifactPath
    - datasetWindow, universeRef

    Steps:
    1) Build dataset rows from Torghut decision/review history and market-context snapshots for `${datasetWindow}`.
    2) Materialize `${artifactPath}/dspy-dataset.json` and `${artifactPath}/dspy-dataset-metadata.json`.
    3) Compute and print dataset hash and row counts by split.
---
apiVersion: agents.proompteng.ai/v1alpha1
kind: ImplementationSpec
metadata:
  name: torghut-dspy-compile-mipro-v1
  namespace: agents
spec:
  contract:
    requiredKeys:
      - repository
      - base
      - head
      - artifactPath
      - datasetRef
      - metricPolicyRef
      - optimizer
  text: |
    Objective: Compile Torghut DSPy program prompts using MIPRO and emit deterministic compile artifacts.

    Guardrails:
    - Compile against `${datasetRef}` only.
    - Emit compile artifacts under `${artifactPath}`.
    - Persist compile metrics and reproducibility hashes.

    Inputs:
    - repository, base, head, artifactPath
    - datasetRef, metricPolicyRef, optimizer

    Steps:
    1) Run DSPy compile with `${optimizer}` and `${metricPolicyRef}`.
    2) Emit `${artifactPath}/dspy-compile-result.json` with artifactHash/reproducibilityHash.
    3) Emit compiled prompt/program artifact referenced by `compiledArtifactUri`.
---
apiVersion: agents.proompteng.ai/v1alpha1
kind: ImplementationSpec
metadata:
  name: torghut-dspy-eval-v1
  namespace: agents
spec:
  contract:
    requiredKeys:
      - repository
      - base
      - head
      - artifactPath
      - compileResultRef
      - gatePolicyRef
  text: |
    Objective: Evaluate compiled DSPy artifact against Torghut policy gates and produce promotion recommendation.

    Guardrails:
    - Block promotion when schema validity, deterministic compatibility, or fallback thresholds fail.
    - Emit deterministic eval report JSON at `${artifactPath}/dspy-eval-report.json`.

    Inputs:
    - repository, base, head, artifactPath
    - compileResultRef, gatePolicyRef

    Steps:
    1) Evaluate compile artifact referenced by `${compileResultRef}`.
    2) Compute schemaValidRate, vetoAlignmentRate, falseVetoRate, latencyP95Ms.
    3) Emit gate compatibility and promotion recommendation in eval report.
---
apiVersion: agents.proompteng.ai/v1alpha1
kind: ImplementationSpec
metadata:
  name: torghut-dspy-gepa-experiment-v1
  namespace: agents
spec:
  contract:
    requiredKeys:
      - repository
      - base
      - head
      - artifactPath
      - baselineArtifactRef
      - experimentName
  text: |
    Objective: Run optional GEPA experimentation for DSPy prompt optimization and publish comparison artifacts.

    Guardrails:
    - Do not promote artifacts from this lane directly.
    - Emit comparison metrics and candidate references only.

    Inputs:
    - repository, base, head, artifactPath
    - baselineArtifactRef, experimentName

    Steps:
    1) Execute GEPA experiment `${experimentName}` against `${baselineArtifactRef}`.
    2) Emit `${artifactPath}/dspy-gepa-report.json` with deltas and reproducibility metadata.
---
apiVersion: agents.proompteng.ai/v1alpha1
kind: ImplementationSpec
metadata:
  name: torghut-dspy-promote-artifact-v1
  namespace: agents
spec:
  contract:
    requiredKeys:
      - repository
      - base
      - head
      - artifactPath
      - evalReportRef
      - artifactHash
      - promotionTarget
      - approvalRef
  text: |
    Objective: Promote an evaluated DSPy artifact to the requested rollout target only when policy gates pass.

    Guardrails:
    - Promotion is blocked unless schema validity >= 99.5%, deterministic compatibility passes,
      and fallback/timeout thresholds are within policy.
    - Emit promotion record at `${artifactPath}/dspy-promotion-record.json`.

    Inputs:
    - repository, base, head, artifactPath
    - evalReportRef, artifactHash, promotionTarget, approvalRef

    Steps:
    1) Validate `${evalReportRef}` satisfies promotion policy for `${promotionTarget}`.
    2) Verify `${artifactHash}` matches compile/eval lineage and reproducibility checks.
    3) Emit approved/blocked promotion record with reasons and policy evidence.
