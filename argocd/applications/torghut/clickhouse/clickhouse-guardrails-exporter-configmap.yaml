apiVersion: v1
kind: ConfigMap
metadata:
  name: torghut-clickhouse-guardrails-exporter
  namespace: torghut
  labels:
    app.kubernetes.io/name: torghut-clickhouse-guardrails-exporter
    app.kubernetes.io/part-of: torghut
data:
  exporter.py: |
    import json
    import os
    import threading
    import time
    import urllib.parse
    import urllib.request
    from http.server import BaseHTTPRequestHandler, HTTPServer

    CLICKHOUSE_URL = os.environ.get("CLICKHOUSE_URL", "http://torghut-clickhouse.torghut.svc.cluster.local:8123")
    CLICKHOUSE_USER = os.environ.get("CLICKHOUSE_USER", "torghut")
    CLICKHOUSE_PASSWORD = os.environ.get("CLICKHOUSE_PASSWORD", "")
    DATABASE = os.environ.get("CLICKHOUSE_DATABASE", "torghut")
    EXPORTER_PORT = int(os.environ.get("EXPORTER_PORT", "9108"))
    SCRAPE_INTERVAL_SECONDS = int(os.environ.get("SCRAPE_INTERVAL_SECONDS", "30"))

    DEFAULT_DISK_NAME = os.environ.get("CLICKHOUSE_DISK_NAME", "default")
    TABLES = os.environ.get("CLICKHOUSE_REPLICATED_TABLES", "ta_signals,ta_microbars").split(",")
    TABLES = [t.strip() for t in TABLES if t.strip()]


    def clickhouse_query(query: str) -> dict:
        params = urllib.parse.urlencode({"query": query})
        url = f"{CLICKHOUSE_URL.rstrip('/')}/?{params}"
        headers = {}
        if CLICKHOUSE_USER:
            headers["X-ClickHouse-User"] = CLICKHOUSE_USER
        if CLICKHOUSE_PASSWORD:
            headers["X-ClickHouse-Key"] = CLICKHOUSE_PASSWORD
        req = urllib.request.Request(url, headers=headers, method="GET")
        with urllib.request.urlopen(req, timeout=10) as resp:
            body = resp.read().decode("utf-8").strip()
            if not body:
                return {}
            return json.loads(body)


    class State:
        def __init__(self):
            self.lock = threading.Lock()
            self.clickhouse_up = 0.0
            self.disk_free_bytes = float("nan")
            self.disk_total_bytes = float("nan")
            self.disk_free_ratio = float("nan")
            self.any_replica_readonly = float("nan")
            self.last_scrape_success = 0.0
            self.last_scrape_ts_seconds = 0.0
            self.last_error = ""

        def snapshot(self) -> dict:
            with self.lock:
                return {
                    "clickhouse_up": self.clickhouse_up,
                    "disk_free_bytes": self.disk_free_bytes,
                    "disk_total_bytes": self.disk_total_bytes,
                    "disk_free_ratio": self.disk_free_ratio,
                    "any_replica_readonly": self.any_replica_readonly,
                    "last_scrape_success": self.last_scrape_success,
                    "last_scrape_ts_seconds": self.last_scrape_ts_seconds,
                }


    state = State()


    def float_or_nan(value):
        try:
            return float(value)
        except Exception:
            return float("nan")


    def scrape_once():
        now = time.time()
        try:
            disk_query = (
                "SELECT max(free_space) AS free_space, max(total_space) AS total_space "
                "FROM system.disks "
                f"WHERE name = {json.dumps(DEFAULT_DISK_NAME)} "
                "FORMAT JSONEachRow"
            )
            disk_row = clickhouse_query(disk_query) or {}

            readonly_query = (
                "SELECT max(is_readonly) AS any_readonly "
                "FROM system.replicas "
                f"WHERE database = {json.dumps(DATABASE)} "
                f"AND table IN ({', '.join(json.dumps(t) for t in TABLES)}) "
                "FORMAT JSONEachRow"
            )
            readonly_row = clickhouse_query(readonly_query) or {}

            free_space = float_or_nan(disk_row.get("free_space"))
            total_space = float_or_nan(disk_row.get("total_space"))
            ratio = float("nan")
            if total_space and total_space > 0 and free_space == free_space:
                ratio = free_space / total_space

            any_readonly = float_or_nan(readonly_row.get("any_readonly"))

            with state.lock:
                state.clickhouse_up = 1.0
                state.disk_free_bytes = free_space
                state.disk_total_bytes = total_space
                state.disk_free_ratio = ratio
                state.any_replica_readonly = any_readonly
                state.last_scrape_success = 1.0
                state.last_scrape_ts_seconds = now
                state.last_error = ""
        except Exception as e:
            with state.lock:
                state.clickhouse_up = 0.0
                state.last_scrape_success = 0.0
                state.last_scrape_ts_seconds = now
                state.last_error = str(e)


    def scrape_loop():
        while True:
            scrape_once()
            time.sleep(SCRAPE_INTERVAL_SECONDS)


    def render_metrics(snapshot: dict) -> str:
        lines = []
        lines.append("# HELP torghut_clickhouse_guardrails_clickhouse_up 1 if ClickHouse is reachable for guardrail queries.")
        lines.append("# TYPE torghut_clickhouse_guardrails_clickhouse_up gauge")
        lines.append(f"torghut_clickhouse_guardrails_clickhouse_up {snapshot['clickhouse_up']}")

        lines.append("# HELP torghut_clickhouse_guardrails_disk_free_bytes Free bytes on the ClickHouse default disk.")
        lines.append("# TYPE torghut_clickhouse_guardrails_disk_free_bytes gauge")
        lines.append(f"torghut_clickhouse_guardrails_disk_free_bytes {snapshot['disk_free_bytes']}")

        lines.append("# HELP torghut_clickhouse_guardrails_disk_total_bytes Total bytes on the ClickHouse default disk.")
        lines.append("# TYPE torghut_clickhouse_guardrails_disk_total_bytes gauge")
        lines.append(f"torghut_clickhouse_guardrails_disk_total_bytes {snapshot['disk_total_bytes']}")

        lines.append("# HELP torghut_clickhouse_guardrails_disk_free_ratio Free/total ratio on the ClickHouse default disk.")
        lines.append("# TYPE torghut_clickhouse_guardrails_disk_free_ratio gauge")
        lines.append(f"torghut_clickhouse_guardrails_disk_free_ratio {snapshot['disk_free_ratio']}")

        lines.append(
            "# HELP torghut_clickhouse_guardrails_any_replica_readonly 1 if any Torghut replicated table is read-only."
        )
        lines.append("# TYPE torghut_clickhouse_guardrails_any_replica_readonly gauge")
        lines.append(f"torghut_clickhouse_guardrails_any_replica_readonly {snapshot['any_replica_readonly']}")

        lines.append("# HELP torghut_clickhouse_guardrails_last_scrape_success 1 if the last scrape succeeded.")
        lines.append("# TYPE torghut_clickhouse_guardrails_last_scrape_success gauge")
        lines.append(f"torghut_clickhouse_guardrails_last_scrape_success {snapshot['last_scrape_success']}")

        lines.append("# HELP torghut_clickhouse_guardrails_last_scrape_ts_seconds Unix timestamp of the last scrape attempt.")
        lines.append("# TYPE torghut_clickhouse_guardrails_last_scrape_ts_seconds gauge")
        lines.append(f"torghut_clickhouse_guardrails_last_scrape_ts_seconds {snapshot['last_scrape_ts_seconds']}")

        lines.append("")
        return "\n".join(lines)


    class Handler(BaseHTTPRequestHandler):
        def do_GET(self):
            if self.path not in ("/metrics", "/"):
                self.send_response(404)
                self.end_headers()
                return

            snapshot = state.snapshot()
            body = render_metrics(snapshot).encode("utf-8")
            self.send_response(200)
            self.send_header("Content-Type", "text/plain; version=0.0.4; charset=utf-8")
            self.send_header("Content-Length", str(len(body)))
            self.end_headers()
            self.wfile.write(body)

        def log_message(self, format, *args):
            return


    if __name__ == "__main__":
        t = threading.Thread(target=scrape_loop, daemon=True)
        t.start()

        server = HTTPServer(("0.0.0.0", EXPORTER_PORT), Handler)
        server.serve_forever()

