apiVersion: v1
kind: ConfigMap
metadata:
  name: torghut-clickhouse-guardrails-exporter
  namespace: torghut
  labels:
    app.kubernetes.io/name: torghut-clickhouse-guardrails-exporter
    app.kubernetes.io/part-of: torghut
data:
  exporter.py: |
    import json
    import os
    import threading
    import time
    import urllib.parse
    import urllib.request
    from http.server import BaseHTTPRequestHandler, HTTPServer

    CLICKHOUSE_URL = os.environ.get("CLICKHOUSE_URL", "http://torghut-clickhouse.torghut.svc.cluster.local:8123")
    CLICKHOUSE_USER = os.environ.get("CLICKHOUSE_USER", "torghut")
    CLICKHOUSE_PASSWORD = os.environ.get("CLICKHOUSE_PASSWORD", "")
    DATABASE = os.environ.get("CLICKHOUSE_DATABASE", "torghut")
    CLICKHOUSE_CLUSTER = os.environ.get("CLICKHOUSE_CLUSTER", "default")
    CLICKHOUSE_NAMESPACE = os.environ.get("CLICKHOUSE_NAMESPACE", "torghut")
    CLICKHOUSE_PORT = int(os.environ.get("CLICKHOUSE_PORT", "8123"))
    EXPORTER_PORT = int(os.environ.get("EXPORTER_PORT", "9108"))
    SCRAPE_INTERVAL_SECONDS = int(os.environ.get("SCRAPE_INTERVAL_SECONDS", "30"))

    DEFAULT_DISK_NAME = os.environ.get("CLICKHOUSE_DISK_NAME", "default")
    TABLES = os.environ.get("CLICKHOUSE_REPLICATED_TABLES", "ta_signals,ta_microbars").split(",")
    TABLES = [t.strip() for t in TABLES if t.strip()]


    def clickhouse_query_rows(base_url: str, query: str) -> list[dict]:
        params = urllib.parse.urlencode({"query": query})
        url = f"{base_url.rstrip('/')}/?{params}"
        headers = {}
        if CLICKHOUSE_USER:
            headers["X-ClickHouse-User"] = CLICKHOUSE_USER
        if CLICKHOUSE_PASSWORD:
            headers["X-ClickHouse-Key"] = CLICKHOUSE_PASSWORD
        req = urllib.request.Request(url, headers=headers, method="GET")
        with urllib.request.urlopen(req, timeout=10) as resp:
            body = resp.read().decode("utf-8").strip()
            if not body:
                return []
            rows = []
            for line in body.splitlines():
                line = line.strip()
                if not line:
                    continue
                rows.append(json.loads(line))
            return rows


    def clickhouse_str(value: str) -> str:
        # ClickHouse string literal quoting (single quotes). Double quotes are identifiers.
        return "'" + value.replace("'", "''") + "'"


    def prom_label_value(value: str) -> str:
        # Minimal escaping for Prometheus label values.
        return value.replace("\\", "\\\\").replace("\n", "\\n").replace('"', '\\"')


    class State:
        def __init__(self):
            self.lock = threading.Lock()
            # replica -> metrics
            self.replicas: dict[str, dict] = {}
            self.last_scrape_success = 0.0
            self.last_scrape_ts_seconds = 0.0
            self.last_error = ""

        def snapshot(self) -> dict:
            with self.lock:
                return {
                    "replicas": dict(self.replicas),
                    "last_scrape_success": self.last_scrape_success,
                    "last_scrape_ts_seconds": self.last_scrape_ts_seconds,
                }


    state = State()


    def float_or_nan(value):
        try:
            return float(value)
        except Exception:
            return float("nan")


    def scrape_once():
        now = time.time()
        try:
            discovery_query = (
                "SELECT host_name AS replica "
                "FROM system.clusters "
                f"WHERE cluster = {clickhouse_str(CLICKHOUSE_CLUSTER)} "
                "GROUP BY replica "
                "ORDER BY replica "
                "FORMAT JSONEachRow"
            )
            replica_rows = clickhouse_query_rows(CLICKHOUSE_URL, discovery_query)
            replica_names = [str(row.get("replica") or "") for row in replica_rows]
            replica_names = [r for r in replica_names if r]
            if not replica_names:
                raise RuntimeError("no ClickHouse replicas discovered via system.clusters")

            disk_query = (
                "SELECT max(free_space) AS free_space, max(total_space) AS total_space "
                "FROM system.disks "
                f"WHERE name = {clickhouse_str(DEFAULT_DISK_NAME)} "
                "FORMAT JSONEachRow"
            )

            readonly_query = ""
            if TABLES:
                readonly_query = (
                    "SELECT max(is_readonly) AS any_readonly "
                    "FROM system.replicas "
                    f"WHERE database = {clickhouse_str(DATABASE)} "
                    f"AND table IN ({', '.join(clickhouse_str(t) for t in TABLES)}) "
                    "FORMAT JSONEachRow"
                )

            replicas: dict[str, dict] = {}
            scrape_errors: list[str] = []

            for replica in replica_names:
                replica_url = f"http://{replica}.{CLICKHOUSE_NAMESPACE}.svc.cluster.local:{CLICKHOUSE_PORT}"
                try:
                    disk_rows = clickhouse_query_rows(replica_url, disk_query)
                    if not disk_rows:
                        raise RuntimeError("disk query returned no rows")
                    disk_row = disk_rows[0]

                    readonly_row: dict = {}
                    if readonly_query:
                        readonly_rows = clickhouse_query_rows(replica_url, readonly_query)
                        if not readonly_rows:
                            raise RuntimeError("readonly query returned no rows")
                        readonly_row = readonly_rows[0]

                    free_space = float_or_nan(disk_row.get("free_space"))
                    total_space = float_or_nan(disk_row.get("total_space"))
                    ratio = float("nan")
                    if total_space and total_space > 0 and free_space == free_space:
                        ratio = free_space / total_space

                    replicas[replica] = {
                        "clickhouse_up": 1.0,
                        "disk_free_bytes": free_space,
                        "disk_total_bytes": total_space,
                        "disk_free_ratio": ratio,
                        "any_replica_readonly": float_or_nan(readonly_row.get("any_readonly", 0.0)),
                    }
                except Exception as e:
                    scrape_errors.append(f"{replica}: {e}")
                    replicas[replica] = {
                        "clickhouse_up": 0.0,
                        "disk_free_bytes": float("nan"),
                        "disk_total_bytes": float("nan"),
                        "disk_free_ratio": float("nan"),
                        "any_replica_readonly": float("nan"),
                    }

            last_scrape_success = 1.0 if scrape_errors == [] else 0.0

            with state.lock:
                state.replicas = replicas
                state.last_scrape_success = last_scrape_success
                state.last_scrape_ts_seconds = now
                state.last_error = "; ".join(scrape_errors)
        except Exception as e:
            with state.lock:
                state.replicas = {}
                state.last_scrape_success = 0.0
                state.last_scrape_ts_seconds = now
                state.last_error = str(e)


    def scrape_loop():
        while True:
            scrape_once()
            time.sleep(SCRAPE_INTERVAL_SECONDS)


    def render_metrics(snapshot: dict) -> str:
        lines = []
        lines.append(
            "# HELP torghut_clickhouse_guardrails_clickhouse_up 1 if ClickHouse is reachable for guardrail queries (per replica)."
        )
        lines.append("# TYPE torghut_clickhouse_guardrails_clickhouse_up gauge")
        for replica, metrics in snapshot["replicas"].items():
            label = prom_label_value(replica)
            lines.append(
                f'torghut_clickhouse_guardrails_clickhouse_up{{replica="{label}"}} {metrics["clickhouse_up"]}'
            )

        lines.append("# HELP torghut_clickhouse_guardrails_disk_free_bytes Free bytes on the ClickHouse default disk (per replica).")
        lines.append("# TYPE torghut_clickhouse_guardrails_disk_free_bytes gauge")
        for replica, metrics in snapshot["replicas"].items():
            label = prom_label_value(replica)
            lines.append(
                f'torghut_clickhouse_guardrails_disk_free_bytes{{replica="{label}"}} {metrics["disk_free_bytes"]}'
            )

        lines.append("# HELP torghut_clickhouse_guardrails_disk_total_bytes Total bytes on the ClickHouse default disk (per replica).")
        lines.append("# TYPE torghut_clickhouse_guardrails_disk_total_bytes gauge")
        for replica, metrics in snapshot["replicas"].items():
            label = prom_label_value(replica)
            lines.append(
                f'torghut_clickhouse_guardrails_disk_total_bytes{{replica="{label}"}} {metrics["disk_total_bytes"]}'
            )

        lines.append(
            "# HELP torghut_clickhouse_guardrails_disk_free_ratio Free/total ratio on the ClickHouse default disk (per replica)."
        )
        lines.append("# TYPE torghut_clickhouse_guardrails_disk_free_ratio gauge")
        for replica, metrics in snapshot["replicas"].items():
            label = prom_label_value(replica)
            lines.append(
                f'torghut_clickhouse_guardrails_disk_free_ratio{{replica="{label}"}} {metrics["disk_free_ratio"]}'
            )

        lines.append(
            "# HELP torghut_clickhouse_guardrails_any_replica_readonly 1 if any Torghut replicated table is read-only (per replica)."
        )
        lines.append("# TYPE torghut_clickhouse_guardrails_any_replica_readonly gauge")
        for replica, metrics in snapshot["replicas"].items():
            label = prom_label_value(replica)
            lines.append(
                f'torghut_clickhouse_guardrails_any_replica_readonly{{replica="{label}"}} {metrics["any_replica_readonly"]}'
            )

        lines.append("# HELP torghut_clickhouse_guardrails_last_scrape_success 1 if the last scrape succeeded.")
        lines.append("# TYPE torghut_clickhouse_guardrails_last_scrape_success gauge")
        lines.append(f"torghut_clickhouse_guardrails_last_scrape_success {snapshot['last_scrape_success']}")

        lines.append("# HELP torghut_clickhouse_guardrails_last_scrape_ts_seconds Unix timestamp of the last scrape attempt.")
        lines.append("# TYPE torghut_clickhouse_guardrails_last_scrape_ts_seconds gauge")
        lines.append(f"torghut_clickhouse_guardrails_last_scrape_ts_seconds {snapshot['last_scrape_ts_seconds']}")

        lines.append("")
        return "\n".join(lines)


    class Handler(BaseHTTPRequestHandler):
        def do_GET(self):
            if self.path not in ("/metrics", "/"):
                self.send_response(404)
                self.end_headers()
                return

            snapshot = state.snapshot()
            body = render_metrics(snapshot).encode("utf-8")
            self.send_response(200)
            self.send_header("Content-Type", "text/plain; version=0.0.4; charset=utf-8")
            self.send_header("Content-Length", str(len(body)))
            self.end_headers()
            self.wfile.write(body)

        def log_message(self, format, *args):
            return


    if __name__ == "__main__":
        t = threading.Thread(target=scrape_loop, daemon=True)
        t.start()

        server = HTTPServer(("0.0.0.0", EXPORTER_PORT), Handler)
        server.serve_forever()
